# ğŸ«€ Há»‡ Thá»‘ng Cháº©n ÄoÃ¡n Bá»‡nh Tim Máº¡ch Báº±ng Ensemble Learning

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> Äá»“ Ã¡n mÃ´n há»c **MÃ¡y há»c**: XÃ¢y dá»±ng há»‡ thá»‘ng cháº©n Ä‘oÃ¡n bá»‡nh tim máº¡ch (Cardiovascular Disease Prediction) sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t há»c mÃ¡y tiÃªn tiáº¿n, táº­p trung vÃ o viá»‡c so sÃ¡nh hiá»‡u quáº£ giá»¯a cÃ¡c **Single Models** vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p **Ensemble Learning** phá»©c táº¡p.

---

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan dá»± Ã¡n](#-tá»•ng-quan-dá»±-Ã¡n)
- [Dataset](#-dataset)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [YÃªu cáº§u há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Quy trÃ¬nh thá»±c hiá»‡n](#-quy-trÃ¬nh-thá»±c-hiá»‡n)
- [Káº¿t quáº£](#-káº¿t-quáº£)
- [Metrics Ä‘Ã¡nh giÃ¡](#-metrics-Ä‘Ã¡nh-giÃ¡)
- [PhÃ¢n tÃ­ch vÃ  Insights](#-phÃ¢n-tÃ­ch-vÃ -insights)
- [TÃ¡c giáº£](#-tÃ¡c-giáº£)

---

## Tá»•ng quan dá»± Ã¡n

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng dá»± Ä‘oÃ¡n bá»‡nh tim máº¡ch sá»­ dá»¥ng Machine Learning, vá»›i má»¥c tiÃªu:

- So sÃ¡nh hiá»‡u quáº£ giá»¯a **Single Models** (Logistic Regression, KNN, Naive Bayes, Decision Tree) vÃ  **Ensemble Methods** (Random Forest, XGBoost, LightGBM, Voting, Stacking)
- Tá»‘i Æ°u hÃ³a hyperparameters sá»­ dá»¥ng **Optuna** vá»›i TPE Sampler
- Xá»­ lÃ½ dá»¯ liá»‡u chuyÃªn sÃ¢u: outlier removal, feature engineering, binning, clustering
- ÄÃ¡nh giÃ¡ toÃ n diá»‡n vá»›i nhiá»u metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC)
- Äáº¡t Ä‘Æ°á»£c **F1-Score: 0.8097** vá»›i mÃ´ hÃ¬nh Stacking Classifier

### Best Model

**Stacking Classifier** Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t:
- **Accuracy**: 82.0%
- **Precision**: 84.3%
- **Recall**: 77.9%
- **F1-Score**: 81.0%
- **ROC-AUC**: 91.0%

---

## Dataset

### ThÃ´ng tin cÆ¡ báº£n

- **Nguá»“n**: Cardiovascular Disease Dataset (`cardio_train.csv`)
- **Sá»‘ lÆ°á»£ng ban Ä‘áº§u**: 70,000 samples
- **Sá»‘ lÆ°á»£ng sau preprocessing**: 60,752 samples (sau khi loáº¡i bá» outliers)
- **Sá»‘ features gá»‘c**: 12 features
- **Sá»‘ features cuá»‘i cÃ¹ng**: 10 features (sau feature engineering vÃ  selection)
- **Target variable**: `cardio` (0: KhÃ´ng bá»‡nh, 1: CÃ³ bá»‡nh tim máº¡ch)
- **Class distribution**: ~50-50 (cÃ¢n báº±ng, khÃ´ng cáº§n xá»­ lÃ½ imbalance)

### Features gá»‘c

| Feature | MÃ´ táº£ | Kiá»ƒu dá»¯ liá»‡u |
|---------|-------|--------------|
| `id` | MÃ£ Ä‘á»‹nh danh bá»‡nh nhÃ¢n | Integer |
| `age` | Tuá»•i (tÃ­nh báº±ng ngÃ y) | Integer |
| `gender` | Giá»›i tÃ­nh (1: Ná»¯, 2: Nam) | Integer |
| `height` | Chiá»u cao (cm) | Integer |
| `weight` | CÃ¢n náº·ng (kg) | Float |
| `ap_hi` | Huyáº¿t Ã¡p tÃ¢m thu (Systolic) | Integer |
| `ap_lo` | Huyáº¿t Ã¡p tÃ¢m trÆ°Æ¡ng (Diastolic) | Integer |
| `cholesterol` | Má»©c cholesterol (1: BÃ¬nh thÆ°á»ng, 2: Cao, 3: Ráº¥t cao) | Integer |
| `gluc` | Má»©c glucose (1: BÃ¬nh thÆ°á»ng, 2: Cao, 3: Ráº¥t cao) | Integer |
| `smoke` | HÃºt thuá»‘c (0: KhÃ´ng, 1: CÃ³) | Integer |
| `alco` | Uá»‘ng rÆ°á»£u (0: KhÃ´ng, 1: CÃ³) | Integer |
| `active` | Hoáº¡t Ä‘á»™ng thá»ƒ cháº¥t (0: KhÃ´ng, 1: CÃ³) | Integer |
| `cardio` | Bá»‡nh tim máº¡ch (0: KhÃ´ng, 1: CÃ³) - **TARGET** | Integer |

### Features sau preprocessing

Sau quÃ¡ trÃ¬nh xá»­ lÃ½, dataset cuá»‘i cÃ¹ng cÃ³ **10 features**:

1. `gender` - Giá»›i tÃ­nh
2. `cholesterol` - Má»©c cholesterol
3. `gluc` - Má»©c glucose
4. `smoke` - HÃºt thuá»‘c
5. `alco` - Uá»‘ng rÆ°á»£u
6. `active` - Hoáº¡t Ä‘á»™ng thá»ƒ cháº¥t
7. `age_bin` - NhÃ³m tuá»•i (binned)
8. `BMI_Class` - PhÃ¢n loáº¡i BMI (binned)
9. `MAP_Class` - PhÃ¢n loáº¡i Mean Arterial Pressure (binned)
10. `cluster` - Cluster tá»« K-Modes clustering

---

## Cáº¥u trÃºc dá»± Ã¡n

```
heart_disease_ensemble_model/
â”‚
â”œâ”€â”€ DataRaw/                          # Dataset gá»‘c
â”‚   â””â”€â”€ cardio_train.csv              # Dataset ban Ä‘áº§u (70,000 samples)
â”‚
â”œâ”€â”€ data/                             # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚   â”œâ”€â”€ X_train_full.npy              # Training set (48,601 samples)
â”‚   â”œâ”€â”€ y_train_full.npy              # Training labels
â”‚   â”œâ”€â”€ X_test.npy                    # Test set (12,151 samples)
â”‚   â”œâ”€â”€ y_test.npy                    # Test labels
â”‚   â”œâ”€â”€ feature_names.npy             # TÃªn cÃ¡c features sau xá»­ lÃ½
â”‚   â””â”€â”€ kfold_indices.npy             # K-Fold indices cho cross-validation
â”‚
â”œâ”€â”€ models/                           # Models Ä‘Ã£ Ä‘Æ°á»£c train
â”‚   â”œâ”€â”€ single_logisticregression.pkl
â”‚   â”œâ”€â”€ single_knn.pkl
â”‚   â”œâ”€â”€ single_naivebayes.pkl
â”‚   â”œâ”€â”€ single_decisiontree.pkl
â”‚   â”œâ”€â”€ ensemble_randomforest.pkl
â”‚   â”œâ”€â”€ ensemble_xgboost.pkl
â”‚   â”œâ”€â”€ ensemble_lightgbm.pkl
â”‚   â”œâ”€â”€ ensemble_voting.pkl
â”‚   â””â”€â”€ ensemble_stacking.pkl         # Best Model
â”‚
â”œâ”€â”€ outputs/                          # Káº¿t quáº£ vÃ  visualizations
â”‚   â”œâ”€â”€ single_models_results.csv     # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ single models
â”‚   â”œâ”€â”€ ensemble_models_results.csv  # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ensemble models
â”‚   â”œâ”€â”€ final_model_summary.csv       # Tá»•ng há»£p táº¥t cáº£ káº¿t quáº£
â”‚   â”œâ”€â”€ confusion_matrix.png          # Confusion matrix cá»§a best model
â”‚   â”œâ”€â”€ roc_curve.png                 # ROC curve cá»§a best model
â”‚   â””â”€â”€ feature_importance.png       # Feature importance (náº¿u cÃ³)
â”‚
â”œâ”€â”€ 01_EDA.ipynb                      # PhÃ¢n tÃ­ch khÃ¡m phÃ¡ dá»¯ liá»‡u
â”œâ”€â”€ 02_Preprocessing.ipynb             # Tiá»n xá»­ lÃ½ vÃ  feature engineering
â”œâ”€â”€ 03_SingleModels.ipynb             # Train vÃ  tune single models
â”œâ”€â”€ 04_EnsembleModels.ipynb           # Train ensemble models
â”œâ”€â”€ 05_Evaluation.ipynb               # ÄÃ¡nh giÃ¡ tá»•ng há»£p vÃ  so sÃ¡nh
â”‚
â”œâ”€â”€ requirements.txt                   # CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
â””â”€â”€ README.md                          # File nÃ y
```

---

## YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8 trá»Ÿ lÃªn
- **RAM**: Tá»‘i thiá»ƒu 4GB (khuyáº¿n nghá»‹ 8GB)
- **Disk space**: ~500MB cho dataset vÃ  models
- **OS**: Windows, Linux, hoáº·c macOS

---

## CÃ i Ä‘áº·t

### 1. Clone repository

```bash
git clone <repository-url>
cd heart_disease_ensemble_model
```

### 2. Táº¡o virtual environment (khuyáº¿n nghá»‹)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 4. CÃ i Ä‘áº·t Jupyter Notebook (náº¿u chÆ°a cÃ³)

```bash
pip install jupyter notebook
```

### 5. Khá»Ÿi cháº¡y Jupyter Notebook

```bash
jupyter notebook
```

---

## Quy trÃ¬nh thá»±c hiá»‡n

Dá»± Ã¡n Ä‘Æ°á»£c chia thÃ nh **5 bÆ°á»›c chÃ­nh**, tÆ°Æ¡ng á»©ng vá»›i 5 notebooks. **Báº¯t buá»™c cháº¡y láº§n lÆ°á»£t theo thá»© tá»±**:

### ğŸ““ 1. Exploratory Data Analysis (01_EDA.ipynb)

**Má»¥c tiÃªu**: Hiá»ƒu rÃµ cáº¥u trÃºc vÃ  Ä‘áº·c Ä‘iá»ƒm cá»§a dataset

**Ná»™i dung**:
- Load vÃ  kiá»ƒm tra tá»•ng quan dá»¯ liá»‡u
- PhÃ¢n tÃ­ch thá»‘ng kÃª mÃ´ táº£ (descriptive statistics)
- PhÃ¢n tÃ­ch phÃ¢n phá»‘i biáº¿n má»¥c tiÃªu (target variable)
- PhÃ¢n tÃ­ch cÃ¡c biáº¿n sá»‘ (numerical features): age, height, weight, ap_hi, ap_lo
- PhÃ¢n tÃ­ch cÃ¡c biáº¿n phÃ¢n loáº¡i (categorical features): gender, cholesterol, gluc, smoke, alco, active
- PhÃ¡t hiá»‡n vÃ  phÃ¢n tÃ­ch outliers (huyáº¿t Ã¡p, chiá»u cao, cÃ¢n náº·ng)
- PhÃ¢n tÃ­ch tÆ°Æ¡ng quan (correlation analysis)
- TÃ­nh toÃ¡n vÃ  phÃ¢n tÃ­ch BMI
- PhÃ¢n tÃ­ch huyáº¿t Ã¡p vÃ  tuá»•i - cÃ¡c yáº¿u tá»‘ quan trá»ng
- Tá»•ng káº¿t vÃ  Ä‘á» xuáº¥t xá»­ lÃ½ cho bÆ°á»›c tiáº¿p theo

**Output**: 
- CÃ¡c biá»ƒu Ä‘á»“ visualization trong `outputs/`
- Dataset vá»›i cÃ¡c features má»›i: `outputs/cardio_eda_explored.csv`

---

### 2. Data Preprocessing (02_Preprocessing.ipynb)

**Má»¥c tiÃªu**: Chuáº©n bá»‹ dá»¯ liá»‡u sáº¡ch vÃ  tá»‘i Æ°u cho machine learning

**Ná»™i dung**:

#### 2.1. Outlier Removal
- Sá»­ dá»¥ng **Quantile Method** (2.5% - 97.5%) Ä‘á»ƒ loáº¡i bá» outliers
- Ãp dá»¥ng cho: `ap_hi`, `ap_lo`, `weight`, `height`
- Káº¿t quáº£: Giáº£m tá»« 70,000 â†’ 60,752 samples

#### 2.2. Feature Engineering
- **Age conversion**: Chuyá»ƒn `age` tá»« ngÃ y sang nÄƒm â†’ `age_in_years`
- **BMI calculation**: `bmi = weight / (height/100)Â²`
- **MAP calculation**: `map = (2 Ã— ap_lo + ap_hi) / 3` (Mean Arterial Pressure)

#### 2.3. Binning (Discretization)
- **Age bins**: [30, 35, 40, 45, 50, 55, 60, 65] â†’ `age_bin`
- **BMI bins**: [0, 18.5, 25, 30, 35, 40, inf] â†’ `BMI_Class`
- **MAP bins**: [0, 70, 80, 90, 100, 110, inf] â†’ `MAP_Class`

#### 2.4. K-Modes Clustering
- Táº¡o feature `cluster` báº±ng K-Modes clustering (k=2)
- TÃ¡ch riÃªng theo gender Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c
- Má»¥c Ä‘Ã­ch: PhÃ¡t hiá»‡n cÃ¡c nhÃ³m bá»‡nh nhÃ¢n cÃ³ Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng

#### 2.5. Data Splitting
- **Train/Test Split**: 80% / 20% (stratified)
  - Training: 48,601 samples
  - Test: 12,151 samples
- **K-Fold Cross-Validation**: 5 folds (StratifiedKFold)

#### 2.6. Save Processed Data
- LÆ°u táº¥t cáº£ dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ o thÆ° má»¥c `data/` dÆ°á»›i dáº¡ng `.npy`

**Output**: 
- `data/X_train_full.npy`, `data/y_train_full.npy`
- `data/X_test.npy`, `data/y_test.npy`
- `data/feature_names.npy`
- `data/kfold_indices.npy`

---

### 3. Single Models Training (03_SingleModels.ipynb)

**Má»¥c tiÃªu**: Huáº¥n luyá»‡n vÃ  tá»‘i Æ°u cÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n láº»

**Ná»™i dung**:

#### 3.1. Models Ä‘Æ°á»£c train
1. **Logistic Regression** (vá»›i StandardScaler)
   - Hyperparameters: `C`, `penalty` (L1/L2)
   - Solver: `saga` (há»— trá»£ cáº£ L1 vÃ  L2)

2. **K-Nearest Neighbors (KNN)** (vá»›i StandardScaler)
   - Hyperparameters: `n_neighbors`, `weights`, `metric`

3. **Naive Bayes (GaussianNB)**
   - Hyperparameters: `var_smoothing`

4. **Decision Tree**
   - Hyperparameters: `max_depth`, `min_samples_split`, `min_samples_leaf`, `criterion`

#### 3.2. Hyperparameter Tuning
- **Tool**: Optuna vá»›i TPE Sampler
- **Trials**: 20 trials per model (cÃ³ thá»ƒ tÄƒng cho production)
- **CV**: 5-fold StratifiedKFold
- **Metric**: F1-Score (primary metric)

#### 3.3. Evaluation
- ÄÃ¡nh giÃ¡ trÃªn test set vá»›i cÃ¡c metrics:
  - Accuracy, Precision, Recall, F1-Score, ROC-AUC
  - Training time

**Output**:
- Models: `models/single_*.pkl`
- Results: `outputs/single_models_results.csv`

**Káº¿t quáº£ Single Models**:

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------|----------|-----------|--------|----------|---------|
| Decision Tree | 81.6% | 84.7% | 76.5% | 80.4% | 90.8% |
| KNN | 80.7% | 82.8% | 76.5% | 79.6% | 89.2% |
| Logistic Regression | 75.3% | 76.7% | 71.6% | 74.0% | 82.5% |
| Naive Bayes | 73.9% | 74.8% | 70.7% | 72.7% | 79.6% |

---

### 4. Ensemble Models Training (04_EnsembleModels.ipynb)

**Má»¥c tiÃªu**: Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ensemble Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t

**Ná»™i dung**:

#### 4.1. Pre-built Ensemble Models
1. **Random Forest**
   - Hyperparameters: `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`
   - Tuning vá»›i Optuna

2. **XGBoost**
   - Hyperparameters: `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`
   - Tuning vá»›i Optuna

3. **LightGBM**
   - Hyperparameters: `n_estimators`, `max_depth`, `learning_rate`, `num_leaves`, `subsample`, `colsample_bytree`
   - Tuning vá»›i Optuna

#### 4.2. True Ensemble Methods
4. **Voting Classifier (Soft Voting)**
   - Káº¿t há»£p 7 base models:
     - Logistic Regression, KNN, Naive Bayes, Decision Tree
     - Random Forest, XGBoost, LightGBM
   - Voting strategy: Soft (sá»­ dá»¥ng predict_proba)

5. **Stacking Classifier**
   - Base estimators: 7 models nhÆ° Voting
   - Meta-learner: Logistic Regression
   - Cross-validation: 5-fold

#### 4.3. Evaluation
- ÄÃ¡nh giÃ¡ tÆ°Æ¡ng tá»± nhÆ° Single Models

**Output**:
- Models: `models/ensemble_*.pkl`
- Results: `outputs/ensemble_models_results.csv`

**Káº¿t quáº£ Ensemble Models**:

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC | Train Time (s) |
|-------|----------|-----------|--------|----------|---------|----------------|
| **Stacking**  | **82.0%** | **84.3%** | **77.9%** | **81.0%** | **91.0%** | 12.86 |
| LightGBM | 81.9% | 84.4% | 77.6% | 80.8% | 91.1% | 0.76 |
| XGBoost | 82.0% | 84.7% | 77.2% | 80.8% | 91.0% | 0.37 |
| Random Forest | 81.8% | 84.3% | 77.3% | 80.7% | 90.9% | 0.56 |
| Voting | 81.5% | 83.6% | 77.5% | 80.5% | 90.4% | 1.99 |

---

### 5. Evaluation & Comparison (05_Evaluation.ipynb)

**Má»¥c tiÃªu**: ÄÃ¡nh giÃ¡ tá»•ng há»£p, so sÃ¡nh vÃ  chá»n best model

**Ná»™i dung**:

#### 5.1. Load vÃ  tá»•ng há»£p káº¿t quáº£
- Load káº¿t quáº£ tá»« `single_models_results.csv` vÃ  `ensemble_models_results.csv`
- Táº¡o báº£ng tá»•ng há»£p táº¥t cáº£ models
- Sáº¯p xáº¿p theo F1-Score

#### 5.2. Best Model Analysis
- XÃ¡c Ä‘á»‹nh best model (Stacking Classifier)
- **Confusion Matrix**: PhÃ¢n tÃ­ch True/False Positives vÃ  Negatives
- **ROC Curve**: Visualize kháº£ nÄƒng phÃ¢n loáº¡i
- **Classification Report**: Chi tiáº¿t Precision, Recall, F1 cho tá»«ng class
- **Feature Importance**: Náº¿u model há»— trá»£ (Random Forest, XGBoost, LightGBM)

#### 5.3. Visualizations
- Bar charts so sÃ¡nh metrics giá»¯a cÃ¡c models
- ROC curves comparison
- Feature importance plots

**Output**:
- `outputs/final_model_summary.csv`
- `outputs/confusion_matrix.png`
- `outputs/roc_curve.png`
- `outputs/feature_importance.png`

---

## Káº¿t quáº£

### So sÃ¡nh Single Models vs Ensemble Models

| Rank | Model | Type | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|------|-------|------|----------|-----------|--------|----------|---------|
| 1 | **Stacking** | Ensemble | **82.0%** | **84.3%** | **77.9%** | **81.0%** | **91.0%** |
| 2 | LightGBM | Ensemble | 81.9% | 84.4% | 77.6% | 80.8% | 91.1% |
| 3 | XGBoost | Ensemble | 82.0% | 84.7% | 77.2% | 80.8% | 91.0% |
| 4 | Random Forest | Ensemble | 81.8% | 84.3% | 77.3% | 80.7% | 90.9% |
| 5 | Voting | Ensemble | 81.5% | 83.6% | 77.5% | 80.5% | 90.4% |
| 6 | Decision Tree | Single | 81.6% | 84.7% | 76.5% | 80.4% | 90.8% |
| 7 | KNN | Single | 80.7% | 82.8% | 76.5% | 79.6% | 89.2% |
| 8 | Logistic Regression | Single | 75.3% | 76.7% | 71.6% | 74.0% | 82.5% |
| 9 | Naive Bayes | Single | 73.9% | 74.8% | 70.7% | 72.7% | 79.6% |

### Best Model Performance (Stacking Classifier)

**Classification Report**:
```
              precision    recall  f1-score   support

           0       0.80      0.86      0.83      6174
           1       0.84      0.78      0.81      5977

    accuracy                           0.82     12151
   macro avg       0.82      0.82      0.82     12151
weighted avg       0.82      0.82      0.82     12151
```

**Confusion Matrix**:
- True Negatives (TN): 5,310
- False Positives (FP): 864
- False Negatives (FN): 1,313
- True Positives (TP): 4,664

---

##  Metrics Ä‘Ã¡nh giÃ¡

Há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ sau Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng mÃ´ hÃ¬nh:

### 1. **Accuracy** (Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ)
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
- Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng trÃªn tá»•ng sá»‘ máº«u
- PhÃ¹ há»£p khi dataset cÃ¢n báº±ng (nhÆ° trong dá»± Ã¡n nÃ y)

### 2. **Precision** (Äá»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n dÆ°Æ¡ng tÃ­nh)
```
Precision = TP / (TP + FP)
```
- Tá»· lá»‡ cÃ¡c ca dá»± Ä‘oÃ¡n cÃ³ bá»‡nh thá»±c sá»± cÃ³ bá»‡nh
- Quan trá»ng khi chi phÃ­ Ä‘iá»u trá»‹ sai cao

### 3. **Recall** (Äá»™ nháº¡y / Sensitivity)
```
Recall = TP / (TP + FN)
```
- Tá»· lá»‡ cÃ¡c ca cÃ³ bá»‡nh Ä‘Æ°á»£c phÃ¡t hiá»‡n Ä‘Ãºng
- **Ráº¥t quan trá»ng trong y táº¿** - giáº£m thiá»ƒu bá» sÃ³t ca bá»‡nh

### 4. **F1-Score** (Chá»‰ sá»‘ chÃ­nh - Primary Metric)
```
F1-Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
- CÃ¢n báº±ng giá»¯a Precision vÃ  Recall
- **Metric chÃ­nh** Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a trong dá»± Ã¡n nÃ y

### 5. **ROC-AUC** (Area Under ROC Curve)
- Kháº£ nÄƒng phÃ¢n biá»‡t giá»¯a 2 classes
- GiÃ¡ trá»‹ cÃ ng cao cÃ ng tá»‘t (tá»‘i Ä‘a = 1.0)
- PhÃ¹ há»£p khi dataset cÃ¢n báº±ng

---

##  PhÃ¢n tÃ­ch vÃ  Insights

### 1. Ensemble Methods vÆ°á»£t trá»™i hÆ¡n Single Models

- **Táº¥t cáº£ 5 ensemble models** Ä‘á»u Ä‘áº¡t F1-Score > 80%
- **Stacking Classifier** Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t (F1: 81.0%)
- **Decision Tree** (single) gáº§n báº±ng má»™t sá»‘ ensemble models, cho tháº¥y tree-based methods phÃ¹ há»£p vá»›i dataset nÃ y

### 2. Feature Engineering cÃ³ tÃ¡c Ä‘á»™ng tÃ­ch cá»±c

- Binning (age_bin, BMI_Class, MAP_Class) giÃºp capture non-linear relationships
- K-Modes clustering táº¡o feature `cluster` pháº£n Ã¡nh cÃ¡c nhÃ³m bá»‡nh nhÃ¢n cÃ³ Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng
- Loáº¡i bá» outliers giÃºp cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»¯ liá»‡u

### 3. Hyperparameter Tuning quan trá»ng

- Sá»­ dá»¥ng Optuna giÃºp tÃ¬m Ä‘Æ°á»£c hyperparameters tá»‘i Æ°u
- Má»—i model cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá»›i tuning phÃ¹ há»£p

### 4. Trade-off giá»¯a Performance vÃ  Training Time

- **Stacking**: Best performance nhÆ°ng training time lÃ¢u nháº¥t (12.86s)
- **XGBoost**: CÃ¢n báº±ng tá»‘t giá»¯a performance vÃ  speed (0.37s)
- **LightGBM**: Performance tá»‘t, training time trung bÃ¬nh (0.76s)

### 5. Yáº¿u tá»‘ quan trá»ng nháº¥t

Tá»« EDA, cÃ¡c yáº¿u tá»‘ cÃ³ tÆ°Æ¡ng quan máº¡nh vá»›i bá»‡nh tim:
- **Tuá»•i** (age): TÆ°Æ¡ng quan dÆ°Æ¡ng máº¡nh (+0.238)
- **Cholesterol**: TÆ°Æ¡ng quan dÆ°Æ¡ng máº¡nh (+0.221)
- **BMI**: TÆ°Æ¡ng quan dÆ°Æ¡ng trung bÃ¬nh (+0.182)
- **Huyáº¿t Ã¡p**: Máº·c dÃ¹ tÆ°Æ¡ng quan yáº¿u nhÆ°ng ráº¥t quan trá»ng trong y táº¿

---

##  CÃ´ng nghá»‡ sá»­ dá»¥ng

### Core Libraries
- **pandas** (â‰¥1.3.0): Data manipulation vÃ  analysis
- **numpy** (â‰¥1.21.0): Numerical computing
- **scikit-learn** (â‰¥1.0.0): Machine learning algorithms vÃ  utilities

### Advanced ML Libraries
- **xgboost** (â‰¥1.5.0): Gradient boosting framework
- **lightgbm** (â‰¥3.3.0): Gradient boosting framework (Microsoft)

### Hyperparameter Optimization
- **optuna** (â‰¥3.0.0): Automated hyperparameter optimization vá»›i TPE Sampler

### Visualization
- **matplotlib** (â‰¥3.4.0): Plotting vÃ  visualization
- **seaborn** (â‰¥0.11.0): Statistical data visualization

### Utilities
- **joblib** (â‰¥1.1.0): Model persistence
- **jupyter** (â‰¥1.0.0): Interactive notebook environment

### Clustering
- **kmodes**: K-Modes clustering cho categorical data

---

##  LÆ°u Ã½ khi sá»­ dá»¥ng

1. **Cháº¡y notebooks theo thá»© tá»±**: Báº¯t buá»™c cháº¡y tá»« 01 â†’ 05 vÃ¬ má»—i notebook phá»¥ thuá»™c vÃ o output cá»§a notebook trÆ°á»›c Ä‘Ã³

2. **TÄƒng sá»‘ trials cho Optuna**: Trong production, nÃªn tÄƒng `N_OPTUNA_TRIALS` tá»« 20 lÃªn 50-100 Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t hÆ¡n

3. **Memory usage**: Khi train Stacking Classifier, cáº§n Ä‘á»§ RAM vÃ¬ nÃ³ train nhiá»u base models

4. **Reproducibility**: Táº¥t cáº£ random seeds Ä‘Ã£ Ä‘Æ°á»£c set (`RANDOM_STATE = 42`) Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ cÃ³ thá»ƒ reproduce

5. **Dataset**: Äáº£m báº£o file `DataRaw/cardio_train.csv` tá»“n táº¡i trÆ°á»›c khi cháº¡y

---

##  HÆ°á»›ng phÃ¡t triá»ƒn

- ThÃªm Neural Networks (MLP, Deep Learning)
- Thá»­ cÃ¡c ensemble methods khÃ¡c (Blending, Boosting variations)
- Feature selection Ä‘á»ƒ giáº£m sá»‘ features
- Deploy model lÃªn web app (Flask/FastAPI)
- Táº¡o API Ä‘á»ƒ predict real-time
- ThÃªm SHAP values Ä‘á»ƒ explainability
- Cross-validation trÃªn toÃ n bá»™ dataset thay vÃ¬ chá»‰ train set

---

##  TÃ¡c giáº£

**NHÃ“M 1**

- Phan Quá»‘c Viá»…n - 23110362
- VÅ© ToÃ n Tháº¯ng - 23110329
- Nguyá»…n Nháº­t Huy - 23110226








