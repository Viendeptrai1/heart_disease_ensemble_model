{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cf47058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Config\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_OPTUNA_TRIALS = 20 # Reduced for speed in demo, increase for production\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"✅ Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876e012",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80c8bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (48601, 10), y_train: (48601,)\n",
      "X_test: (12151, 10), y_test: (12151,)\n",
      "Features: ['gender' 'cholesterol' 'gluc' 'smoke' 'alco' 'active' 'age_bin'\n",
      " 'BMI_Class' 'MAP_Class' 'cluster']\n"
     ]
    }
   ],
   "source": [
    "# Load data from .npy files\n",
    "data_path = 'data'\n",
    "X_train = np.load(f'{data_path}/X_train_full.npy')\n",
    "y_train = np.load(f'{data_path}/y_train_full.npy')\n",
    "X_test = np.load(f'{data_path}/X_test.npy')\n",
    "y_test = np.load(f'{data_path}/y_test.npy')\n",
    "feature_names = np.load(f'{data_path}/feature_names.npy', allow_pickle=True)\n",
    "\n",
    "# Load K-Fold indices (optional, but good for consistency if we wanted to use specific folds)\n",
    "# For Optuna cross_val_score with StratifiedKFold(shuffle=True, seed=42), it replicates the split if N_SPLITS matches.\n",
    "# We will use a fresh StratifiedKFold with same seed for simplicity in Pipeline integration.\n",
    "fold_indices = np.load(f'{data_path}/kfold_indices.npy', allow_pickle=True).item()\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6439a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'roc_auc': roc_auc_score(y, y_proba) if y_proba is not None else None\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics, name):\n",
    "    print(f\"\\n{'='*40}\\n{name}\\n{'='*40}\")\n",
    "    for k, v in metrics.items():\n",
    "        if v: print(f\"{k:<15}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46976d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "best_params = {}\n",
    "\n",
    "def optimize(objective, n_trials=N_OPTUNA_TRIALS):\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bd47c",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression (Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f51b2c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Logistic Regression...\n",
      "Best F1: 0.7351\n",
      "Params: {'C': 0.014936568554617643, 'penalty': 'l1'}\n",
      "\n",
      "========================================\n",
      "LogisticRegression\n",
      "========================================\n",
      "accuracy       : 0.7530\n",
      "precision      : 0.7667\n",
      "recall         : 0.7157\n",
      "f1             : 0.7403\n",
      "roc_auc        : 0.8251\n",
      "train_time     : 0.0784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/single_logisticregression.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def obj_lr(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.01, 10.0, log=True),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'solver': 'saga',\n",
    "        'max_iter': 1000,\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    \n",
    "    # Pipeline with Scaler\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(**params))\n",
    "    ])\n",
    "    \n",
    "    return cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "\n",
    "print(\"Tuning Logistic Regression...\")\n",
    "study_lr = optimize(obj_lr)\n",
    "print(f\"Best F1: {study_lr.best_value:.4f}\")\n",
    "print(f\"Params: {study_lr.best_params}\")\n",
    "\n",
    "# Retrain\n",
    "lr_params = study_lr.best_params\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(**lr_params, solver='saga', max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "start = time.time()\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "train_time = time.time() - start\n",
    "\n",
    "metrics_lr = evaluate_model(lr_pipeline, X_test, y_test)\n",
    "metrics_lr['train_time'] = train_time\n",
    "metrics_lr['tune_time'] = 0 # Simplified\n",
    "results['LogisticRegression'] = metrics_lr\n",
    "trained_models['LogisticRegression'] = lr_pipeline\n",
    "best_params['LogisticRegression'] = lr_params\n",
    "\n",
    "print_metrics(metrics_lr, 'LogisticRegression')\n",
    "joblib.dump(lr_pipeline, 'models/single_logisticregression.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f8830",
   "metadata": {},
   "source": [
    "## 3. KNN (Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "526e1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning KNN...\n",
      "Best F1: 0.7958\n",
      "\n",
      "========================================\n",
      "KNN\n",
      "========================================\n",
      "accuracy       : 0.8066\n",
      "precision      : 0.8285\n",
      "recall         : 0.7653\n",
      "f1             : 0.7956\n",
      "roc_auc        : 0.8924\n",
      "train_time     : 0.0176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/single_knn.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def obj_knn(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 3, 30),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan']),\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', KNeighborsClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "\n",
    "print(\"Tuning KNN...\")\n",
    "study_knn = optimize(obj_knn)\n",
    "print(f\"Best F1: {study_knn.best_value:.4f}\")\n",
    "\n",
    "knn_params = study_knn.best_params\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsClassifier(**knn_params, n_jobs=-1))\n",
    "])\n",
    "start = time.time()\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "train_time = time.time() - start\n",
    "\n",
    "metrics_knn = evaluate_model(knn_pipeline, X_test, y_test)\n",
    "metrics_knn['train_time'] = train_time\n",
    "metrics_knn['tune_time'] = 0\n",
    "results['KNN'] = metrics_knn\n",
    "trained_models['KNN'] = knn_pipeline\n",
    "best_params['KNN'] = knn_params\n",
    "\n",
    "print_metrics(metrics_knn, 'KNN')\n",
    "joblib.dump(knn_pipeline, 'models/single_knn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf23fd7",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2168d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Naive Bayes...\n",
      "Best F1: 0.7268\n",
      "\n",
      "========================================\n",
      "NaiveBayes\n",
      "========================================\n",
      "accuracy       : 0.7385\n",
      "precision      : 0.7475\n",
      "recall         : 0.7074\n",
      "f1             : 0.7269\n",
      "roc_auc        : 0.7959\n",
      "train_time     : 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/single_naivebayes.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def obj_nb(trial):\n",
    "    params = {\n",
    "        'var_smoothing': trial.suggest_float('var_smoothing', 1e-12, 1e-6, log=True)\n",
    "    }\n",
    "    # No scaler needed usually for NB, but safe to add or skip. \n",
    "    # GaussianNB assumes features are gaussian. \n",
    "    # K-Modes features are categorical integers, which is not ideal for GaussianNB.\n",
    "    # But we stick to the required model list.\n",
    "    model = GaussianNB(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "\n",
    "print(\"Tuning Naive Bayes...\")\n",
    "study_nb = optimize(obj_nb, n_trials=10)\n",
    "print(f\"Best F1: {study_nb.best_value:.4f}\")\n",
    "\n",
    "nb_model = GaussianNB(**study_nb.best_params)\n",
    "start = time.time()\n",
    "nb_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start\n",
    "\n",
    "metrics_nb = evaluate_model(nb_model, X_test, y_test)\n",
    "metrics_nb['train_time'] = train_time\n",
    "metrics_nb['tune_time'] = 0\n",
    "results['NaiveBayes'] = metrics_nb\n",
    "trained_models['NaiveBayes'] = nb_model\n",
    "best_params['NaiveBayes'] = study_nb.best_params\n",
    "\n",
    "print_metrics(metrics_nb, 'NaiveBayes')\n",
    "joblib.dump(nb_model, 'models/single_naivebayes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8e852",
   "metadata": {},
   "source": [
    "## 5. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52f39f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Decision Tree...\n",
      "Best F1: 0.8054\n",
      "\n",
      "========================================\n",
      "DecisionTree\n",
      "========================================\n",
      "accuracy       : 0.8163\n",
      "precision      : 0.8468\n",
      "recall         : 0.7649\n",
      "f1             : 0.8038\n",
      "roc_auc        : 0.9084\n",
      "train_time     : 0.0205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/single_decisiontree.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def obj_dt(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "\n",
    "print(\"Tuning Decision Tree...\")\n",
    "study_dt = optimize(obj_dt)\n",
    "print(f\"Best F1: {study_dt.best_value:.4f}\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier(**study_dt.best_params, random_state=RANDOM_STATE)\n",
    "start = time.time()\n",
    "dt_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start\n",
    "\n",
    "metrics_dt = evaluate_model(dt_model, X_test, y_test)\n",
    "metrics_dt['train_time'] = train_time\n",
    "metrics_dt['tune_time'] = 0\n",
    "results['DecisionTree'] = metrics_dt\n",
    "trained_models['DecisionTree'] = dt_model\n",
    "best_params['DecisionTree'] = study_dt.best_params\n",
    "\n",
    "print_metrics(metrics_dt, 'DecisionTree')\n",
    "joblib.dump(dt_model, 'models/single_decisiontree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c9940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to outputs/single_models_results.csv\n",
      "                    accuracy  precision    recall        f1   roc_auc  \\\n",
      "LogisticRegression  0.753024   0.766667  0.715744  0.740331  0.825078   \n",
      "KNN                 0.806600   0.828473  0.765267  0.795617  0.892380   \n",
      "NaiveBayes          0.738540   0.747525  0.707378  0.726898  0.795881   \n",
      "DecisionTree        0.816311   0.846823  0.764932  0.803797  0.908379   \n",
      "\n",
      "                    train_time  tune_time  \n",
      "LogisticRegression    0.078356        0.0  \n",
      "KNN                   0.017610        0.0  \n",
      "NaiveBayes            0.004706        0.0  \n",
      "DecisionTree          0.020549        0.0  \n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.to_csv('outputs/single_models_results.csv')\n",
    "print(\"✅ Results saved to outputs/single_models_results.csv\")\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
